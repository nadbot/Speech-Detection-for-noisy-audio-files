{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Emotion Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "89db950d8a02450bbde296420f0ea758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e991af8d2360420d9348cb069fbf8935",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cb704324e48c475c89fb770ff68b2c76",
              "IPY_MODEL_4f1f298290cf414f8648ae1e755443be",
              "IPY_MODEL_d656d0a8462b4a27ba86677ba68e1939"
            ]
          }
        },
        "e991af8d2360420d9348cb069fbf8935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb704324e48c475c89fb770ff68b2c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_77b490f5669b48d4941d10420a501342",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83c8ceccb894496cb7e90af5975e50e1"
          }
        },
        "4f1f298290cf414f8648ae1e755443be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1ba901ca8d548b4b512ff4decdacea3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 163,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 163,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d7b0416530e499daaf7463763c5e34a"
          }
        },
        "d656d0a8462b4a27ba86677ba68e1939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_580e7f0bd98043af9fbd29e0021f3a6e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 163/163 [00:00&lt;00:00, 4.89kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d635eb2b8443433b9adf21720fe7a10f"
          }
        },
        "77b490f5669b48d4941d10420a501342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83c8ceccb894496cb7e90af5975e50e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1ba901ca8d548b4b512ff4decdacea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d7b0416530e499daaf7463763c5e34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "580e7f0bd98043af9fbd29e0021f3a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d635eb2b8443433b9adf21720fe7a10f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16533f9a0292401f827112f760bb7fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1447e608cfe94b31b8a8618fe4412ef4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bbde0ba842c54498ab6e4198259c8f81",
              "IPY_MODEL_5fd109058bab4f84bc0f54eb72d34dc5",
              "IPY_MODEL_863c89eef92a43a38837c5c4980d2e5e"
            ]
          }
        },
        "1447e608cfe94b31b8a8618fe4412ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbde0ba842c54498ab6e4198259c8f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_af0118557c8148f2af5f4bff0bd4d297",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_691a60da77bb4b0f8186f98f361e9d71"
          }
        },
        "5fd109058bab4f84bc0f54eb72d34dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22ea46d2de964effb103b7748344c727",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 291,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 291,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50932d2cbd8c42508b505c3f8a624871"
          }
        },
        "863c89eef92a43a38837c5c4980d2e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_54348acff36f4f298fc785ca35e2b1a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 291/291 [00:00&lt;00:00, 7.75kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15ce44646b49405e967d333adc12e958"
          }
        },
        "af0118557c8148f2af5f4bff0bd4d297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "691a60da77bb4b0f8186f98f361e9d71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22ea46d2de964effb103b7748344c727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50932d2cbd8c42508b505c3f8a624871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54348acff36f4f298fc785ca35e2b1a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15ce44646b49405e967d333adc12e958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c81d123ed594f12a19456bfa6f253f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_765c9584e0064446bf4807ab3731c70f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ba79ac5bc73426589242d2c87d6eaf9",
              "IPY_MODEL_d78053385bc842d692c1a5f750f54568",
              "IPY_MODEL_026b09b84fcb491b81154940aa057d58"
            ]
          }
        },
        "765c9584e0064446bf4807ab3731c70f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ba79ac5bc73426589242d2c87d6eaf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5c9f5e4ceff475eba6c8ad6e471eb52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_002cb2e1e7fc4e5ba5200a6a4d28fa95"
          }
        },
        "d78053385bc842d692c1a5f750f54568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9372650999c748ce915075f57a5553d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 85,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 85,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbddd93b3806499d8566229f0bd70236"
          }
        },
        "026b09b84fcb491b81154940aa057d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_67547ba68842451db20c3f4be07c6ade",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 85.0/85.0 [00:00&lt;00:00, 2.47kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89dfa7280afd4dd2a35aac25179a89f3"
          }
        },
        "a5c9f5e4ceff475eba6c8ad6e471eb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "002cb2e1e7fc4e5ba5200a6a4d28fa95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9372650999c748ce915075f57a5553d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbddd93b3806499d8566229f0bd70236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67547ba68842451db20c3f4be07c6ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89dfa7280afd4dd2a35aac25179a89f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Wav2Vec 2.0** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by Alexei Baevski, Michael Auli, and Alex Conneau.\n",
        "\n",
        "![Wav2vec Architecture](https://smilegate.ai/wp-content/uploads/2020/08/w2v2-3-800x533.jpg)\n",
        "\n",
        "The contextualized representation of the model can be used for a variety of tasks. On HuggingFace, the section [Wav2vec](https://huggingface.co/docs/transformers/model_doc/wav2vec2) describes how the basic Wav2vec model works and what parameters are set to by default.\n",
        "\n",
        "An introduction on how to use Wav2vec can be found at [Fine-tune Wav2vec](https://huggingface.co/blog/fine-tune-wav2vec2-english)."
      ],
      "metadata": {
        "id": "bwNhSMnTZQce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, a downstream model is created to use Wav2vec for Speech Emotion Recognition\n",
        "\n",
        "The model is trained and tested on various locations and SNR values between SNR+05dB and SNR-20dB on the [QUT-NOISE](https://research.qut.edu.au/saivt/databases/qut-noise-databases-and-protocols/) dataset using RAVDESS for the speech segments.\n",
        "\n",
        "For training recording session A is used, for testing the recording session B. "
      ],
      "metadata": {
        "id": "mz-a4GCeZS_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Paths"
      ],
      "metadata": {
        "id": "yJ6LBzw0ZvYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_files = \"drive/MyDrive/Master Thesis/\"\n",
        "path_python_files = path_files + 'Python_files'"
      ],
      "metadata": {
        "id": "O-UZfxdNZQRF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "cZjcQtcZZyG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xVHjjqVa8j9x"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "!pip install mlflow --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "import numpy as np\n",
        "\n",
        "from packaging import version\n",
        "import pexpect\n",
        "import pickle\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    is_apex_available,\n",
        "    Wav2vec2Processor\n",
        ")\n",
        "from transformers.file_utils import ModelOutput\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
        "    Wav2Vec2PreTrainedModel,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Union, Tuple"
      ],
      "metadata": {
        "id": "zsUi7tgZwPOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(path_python_files)\n",
        "from configs_private import USERNAME, PASSWORD, EXPERIMENT_URL  # credentials for databricks\n",
        "from Enums import AudioClipFileLength, FeatureType, RecPlace, SNR\n",
        "from get_files import get_available_files\n",
        "from Parameters import Parameters\n",
        "from process_frames_wav2vec import get_processed_frames\n",
        "from create_features_and_labels import get_feature_and_labels\n",
        "from plot_files import plot_file_distribution\n",
        "from evaluation_tensorflow import Evaluation"
      ],
      "metadata": {
        "id": "u7OrcaJAZ5Mt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set values of Experiment\n",
        "Set the different values depending on the type of experiment that should be run. When running a sweep of experiments, the values can be adjusted here."
      ],
      "metadata": {
        "id": "n96zsOh4aFDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snr = SNR.MINUS_FIVE  # SNR value of the current experiment\n",
        "labels_per_frame = False  # otherwise labels per window\n",
        "# Type of feature, either raw audio, MFCC, or Spectrogram\n",
        "feature = FeatureType.SPECTROGRAM\n",
        "# if True, ignores raw audio during feature extraction to reduce memory needed\n",
        "# only needed if raw_audio is feature of model is Wav2vec\n",
        "ignore_raw_audio = False\n",
        "# which VAD predictions to use (can be Wav2vec, Unet, Perfect, or None)\n",
        "VAD_predictions = 'None'\n",
        "# Number of epochs to train on\n",
        "train_epochs = 10\n",
        "# split between train and validation set\n",
        "validation_split = 0.3\n",
        "# Batch size of the training\n",
        "batch_size = 32\n",
        "# If the model should stop early if it is not improving\n",
        "early_stopping = False\n",
        "# number of epochs it will wait before stopping early\n",
        "patience = 10\n",
        "# optimizer used to train the model (default for all models is Adam)\n",
        "optimizer='Adam'\n",
        "# loss function (default is BinaryCrossEntropy, but Hingeloss was also tried)\n",
        "loss='BinaryCrossentropy'\n",
        "\n",
        "# Metrics on which to judge the model\n",
        "metrics = ['accuracy']\n",
        "# if save_best, the best trained epoch is loaded after training\n",
        "save_best = True\n",
        "\n",
        "# number of files for training/testing, can be set to reduce memory\n",
        "# all files are shuffled before, so it will be split over all categories\n",
        "nr_train_files = 100\n",
        "nr_test_files = 10"
      ],
      "metadata": {
        "id": "3WZ_LsANZ6p3"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to track results\n",
        "Currently using the databricks community edition to track experiments"
      ],
      "metadata": {
        "id": "QI8hIBn_ad8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Databricks community version does not allow token generation so instead \n",
        "# automatically insert the username and password using expect\n",
        "child = pexpect.spawn('databricks configure --host https://community.cloud.databricks.com', encoding='utf-8')\n",
        "child.expect('Username:')\n",
        "child.sendline(USERNAME)\n",
        "child.expect('Password:')\n",
        "child.sendline(PASSWORD)\n",
        "child.expect('Repeat for confirmation:')\n",
        "child.sendline(PASSWORD)\n",
        "print('Connected to Databricks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0XqBRWNaMcY",
        "outputId": "505978d8-7652-4dfa-d69e-d963380954dd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Databricks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "# Create one Experiment per SNR value (from n-20dB to n+05dB in steps of 5)\n",
        "mlflow.set_experiment(EXPERIMENT_URL+snr.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tucHLhmkagt5",
        "outputId": "e9546a11-3dc3-4792-b015-dc00de4cc720"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/1010535410834463', experiment_id='1010535410834463', lifecycle_stage='active', name='/Users/d.hamandouche@students.uu.nl/n-05', tags={'mlflow.experiment.sourceName': '/Users/d.hamandouche@students.uu.nl/n-05',\n",
              " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
              " 'mlflow.ownerEmail': 'd.hamandouche@students.uu.nl',\n",
              " 'mlflow.ownerId': '6054914286087382'}>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.tensorflow.autolog()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZVK4-41ah9I",
        "outputId": "aac5b5b6-2293-43dd-987a-3a438a70d9a9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022/02/23 12:38:44 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ],
      "metadata": {
        "id": "Iy3KXD8CakcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wav_files_train = get_available_files(path_files+\"RAVDESS/\", snr, AudioClipFileLength.ONE_MINUTE, rec_place=RecPlace.a)\n",
        "wav_files_test = get_available_files(path_files+\"RAVDESS/\", snr, AudioClipFileLength.ONE_MINUTE, rec_place=RecPlace.b) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEkSGw4OajDS",
        "outputId": "771f3a40-1e31-4255-ef65-5cc858c6a744"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length: 400\n",
            "Total length: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the list of audio files to avoid having just one category\n",
        "random.shuffle(wav_files_train)\n",
        "random.shuffle(wav_files_test)"
      ],
      "metadata": {
        "id": "hZCu-Xgtal02"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set all parameters as wanted\n",
        "parameter = Parameters()\n",
        "# set_params_specs is the default for Spectrogram\n",
        "if feature == FeatureType.SPECTROGRAM or feature == FeatureType.RAW_AUDIO:\n",
        "    print('Using spectrogram or raw audio parameter')\n",
        "    parameter.set_params_specs()\n",
        "else:\n",
        "    print('Using MFCC parameter')\n",
        "    parameter.set_params_unet()\n",
        "parameter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZfWqBl0avTv",
        "outputId": "5068efd4-bb4e-43b1-938b-d5f735f152e3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using spectrogram or raw audio parameter\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameters(window_size=400, sliding_window_size=40960, hop_length=320, n_mfcc=32, n_mels=32, n_fft=400, fmin=0, fmax=8000, sr=16000, htk=False, center=True, wiener_filters=12, use_wiener_filter=False, feature_type=<FeatureType.SPECTROGRAM: 'Spectrogram'>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Dataset"
      ],
      "metadata": {
        "id": "VIMI3Kp1ezsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_train = nr_train_files  # len(wav_files_train)\n",
        "x_train, y_train, frames_pos_train, frames_train = get_feature_and_labels(\n",
        "    wav_files_train[0:end_train], parameter, ignore_raw_audio=ignore_raw_audio, \n",
        "    emotion=True)\n",
        "plot_file_distribution(wav_files_train[0:end_train], y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "0lCZr3r2a46j",
        "outputId": "dda3ada7-4e75-4d02-a0fc-342c08754572"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [09:40<00:00,  5.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'n-05': 100})\n",
            "Counter({0: 4396, 2: 45, 4: 39, 3: 33, 1: 30, 7: 27, 5: 15, 6: 15})\n",
            "Counter({'sA': 100})\n",
            "Counter({'CAFE-FOODCOURTB-2': 8, 'CAR-WINUPB-2': 8, 'STREET-KG-1': 8, 'CAFE-CAFE-2': 7, 'REVERB-POOL-1': 7, 'STREET-CITY-1': 7, 'HOME-LIVINGB-1': 6, 'HOME-LIVINGB-2': 6, 'CAR-WINUPB-1': 5, 'STREET-CITY-2': 5, 'REVERB-CARPARK-2': 5, 'STREET-KG-2': 5, 'HOME-KITCHEN-2': 4, 'CAFE-CAFE-1': 4, 'CAR-WINDOWNB-2': 4, 'CAR-WINDOWNB-1': 3, 'CAFE-FOODCOURTB-1': 3, 'REVERB-POOL-2': 3, 'REVERB-CARPARK-1': 2})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEtCAYAAAALNduYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZgdRbXAfyd7IIEQMmEfEpDHFtYECAiyQ0LYRBCUsIkGXNhdQFAWASPIJqgQEEU2eYDsLiAiiCw+wiKbiAsqm+J7T1Gf7ymm3h/ntLfmTt+Ze2/19J2ZPr/v62+m+3Z1V9epOl116tRpCSHgOI7jDH9GdDoDjuM4Tjm4wnccx6kIrvAdx3Eqgit8x3GciuAK33EcpyK4wnccx6kIozqdgb6YMmVKmDZtWqez4TiOM2RYvHjxH0IIXXm/DWqFP23aNB577LFOZ8NxHGfIICK/bvSbm3Qcx3Eqgit8x3GciuAK33EcpyK4wnccx6kIpSp8ETlORJ4VkWdE5HoRGVfm/R3HcapMaQpfRFYBjgZmhRBmACOBA8q6v+M4TtUp26QzChgvIqOApYBXS76/4zhOZSnNDz+E8IqIfB74DfA34O4Qwt3154nIAmABQHd3d1nZG5ZMO/GultO8tHDeAOSkM1T9+R3F60GNMk06ywF7AdOBlYGlRWR+/XkhhEUhhFkhhFldXbmLxRzHcZw2KNOksxPwqxDCGyGEfwDfBLYq8f6O4ziVpkyF/xtgtogsJSIC7Ag8X+L9HcdxKk1pCj+E8ChwE/A48LTde1FZ93ccx6k6pQZPCyGcCpxa5j0dx3EcxVfaOo7jVARX+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBWhzE8cri0iT0bbmyJybFn3dxzHqTplfsT8BWBjABEZCbwC3FLW/R3HcapOp0w6OwK/CCH8ukP3dxzHqRylfvEq4gDg+rwfRGQBsACgu7u77RtMO/GultO8tHBeYemddFwGjlMspffwRWQMsCdwY97vIYRFIYRZIYRZXV1d5WbOcRxnGNMJk85c4PEQwu86cG/HcZzK0gmF/x4amHMcx3GcgaNUhS8iSwM7A98s876O4zhOyZO2IYS/AsuXeU/HcRxH8ZW2juM4FcEVvuM4TkVwhe84jlMRXOE7juNUBFf4juM4FcEVvuM4TkVwhe84jlMRXOE7juNUBFf4juM4FcEVvuM4TkVwhe84jlMRXOE7juNUBFf4juM4FcEVvuM4TkUoOx7+JBG5SUR+KiLPi8iWZd7fcRynypT9EfOLgO+EEPa1b9suVfL9HcdxKktpCl9ElgXeARwKEEL4O/D3su7vOI5Tdco06UwH3gC+KiJPiMgV9slDx3EcpwTKNOmMAjYFjgohPCoiFwEnAp+KTxKRBcACgO7u7hKz5zi9mXbiXS2neWnhvAHISXVxGRRHmT38l4GXQwiP2v5N6AugByGERSGEWSGEWV1dXSVmz3EcZ3hTmsIPIbwO/FZE1rZDOwLPlXV/x3GcqlO2l85RwLXmofNL4LCS7+84jlNZSlX4IYQngVll3tNxHMdRfKWt4zhORXCF7ziOUxFc4TuO41QEV/iO4zgVwRW+4zhORXCF7ziOUxFc4TuO41QEV/iO4zgVwRW+4zhORXCF7ziOUxFc4TuO41QEV/iO4zgVwRW+4zhORXCF7ziOUxFKDY8sIi8Bfwb+CbwVQvBQyY7jOCVR9gdQALYPIfyhA/d1HMepNG7ScRzHqQhlK/wA3C0ii0VkQcn3dhzHqTRlm3S2DiG8IiJTgXtE5KchhAfiE+xFsACgu7u75OwVx7QT72o5zUsL5xWW3hkeeD1yiqTUHn4I4RX7+3vgFmDznHMWhRBmhRBmdXV1lZk9x3GcYU1pCl9ElhaRidn/wC7AM2Xd33Ecp+qUadJZAbhFRLL7XhdC+E6J93ccx6k0pSn8EMIvgY3Kup/jOI7TE3fLdBzHqQiu8B3HcSqCK3zHcZyK4ArfcRynIrjCdxzHqQiu8B3HcSqCK3zHcZyK4ArfcRynIrjCdxzHqQiu8B3HcSqCK3zHcZyK4ArfcRynIrjCdxzHqQiu8B3HcSqCK3zHcZyKULrCF5GRIvKEiNxZ9r0dx3GqTCd6+McAz3fgvo7jOJWmVIUvIqsC84Aryryv4ziOU+43bQEuBD4OTGx0gogsABYAdHd3l5QtJ49pJ97VcpqXFs4rLL0z9PE6MLgorYcvIrsDvw8hLO7rvBDCohDCrBDCrK6urpJy5ziOM/wp06TzdmBPEXkJ+Aawg4hcU+L9HcdxKk1pCj+EcFIIYdUQwjTgAOD7IYT5Zd3fcRyn6rgfvuM4TkUoe9IWgBDCD4AfdOLejuM4VcV7+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FKPMTh+NE5Mci8pSIPCsip5d1b8dxHKfcePj/B+wQQviLiIwGHhSRb4cQHikxD47jOJWlNIUfQgjAX2x3tG2hrPs7juNUnVK/eCUiI4HFwNuAL4YQHs05ZwGwAKC7u7vM7DmOU8e0E+9qOc1LC+cNQE46R2oZDKYyLHXSNoTwzxDCxsCqwOYiMiPnnEUhhFkhhFldXV1lZs9xHGdY0xEvnRDCH4H7gDmduL/jOE4VKdNLp0tEJtn/44GdgZ+WdX/HcZyqU6YNfyXgKrPjjwD+PYRwZ4n3dxzHqTRleun8BNikrPs5juM4PfGVto7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FcIXvOI5TEVzhO47jVARX+I7jOBXBFb7jOE5FKPMDKKuJyH0i8pyIPCsix5R1b8dxHKfcD6C8BZwQQnhcRCYCi0XknhDCcyXmwXEcp7KU1sMPIbwWQnjc/v8z8DywSln3dxzHqTodseGLyDT061ePduL+juM4VaRMkw4AIjIBuBk4NoTwZs7vC4AFAN3d3SXnznGKZdqJd7Wc5qWF8wYgJ45Tcg9fREajyv7aEMI3884JISwKIcwKIczq6uoqM3uO4zjDmjK9dAT4CvB8COH8su7rOI7jKGX28N8OHATsICJP2rZbifd3HMepNKXZ8EMIDwJS1v0cx3GcnvhKW8dxnIrgCt9xHKciuMJ3HMepCK7wHcdxKoIrfMdxnIrgCt9xHKciuMJ3HMepCK7wHcdxKoIrfMdxnIrgCt9xHKciuMJ3HMepCK7wHcdxKoIrfMdxnIrgCt9xHKciuMJ3HMepCGV+8epKEfm9iDxT1j0dx3GcGmX28L8GzCnxfo7jOE5EaQo/hPAA8F9l3c9xHMfpyaCz4YvIAhF5TEQee+ONNzqdHcdxnGHDoFP4IYRFIYRZIYRZXV1dnc6O4zjOsGHQKXzHcRxnYHCF7ziOUxHKdMu8HngYWFtEXhaRw8u6t+M4jgOjyrpRCOE9Zd3LcRzH6Y2bdBzHcSqCK3zHcZyK4ArfcRynIrjCdxzHqQiu8B3HcSqCK3zHcZyK4ArfcRynIrjCdxzHqQiu8B3HcSqCK3zHcZyK4ArfcRynIrjCdxzHqQiu8B3HcSqCK3zHcZyKUKrCF5E5IvKCiPxcRE4s896O4zhVp8wPoIwEvgjMBdYD3iMi65V1f8dxnKpTZg9/c+DnIYRfhhD+DnwD2KvE+zuO41QaCSGUcyORfYE5IYT32/5BwBYhhI/UnbcAWGC7awMvFJyVKcAfOnwNT+8y6HT6wZCHoZ5+sOShntVDCF15P5T2icNmCSEsAhYN1PVF5LEQwqxOXsPTuww6nX4w5GGopx8seWiFMk06rwCrRfur2jHHcRynBMpU+P8BrCUi00VkDHAAcHuJ93ccx6k0pZl0QghvichHgO8CI4ErQwjPlnX/iCLMRanX8PTpdDoPQz39YMjDUE8/WPLQNKVN2jqO4zidxVfaOo7jVARX+I7jOBXBFb7jDDNERDqdB2dwMiwVficrfFH3rnqj7fTzd/r+iQzLdt0q7cowS1dEHRhs9WjQLbxqFxHpAkaGEF4PIQQRkdDmjLSIjAghLGnj/v8VQvhnu/cWkbWBJcBfQwivtpoPEdkA6AZ+ArwWQnirjTxsAIwF/hRCeLHFtGNCCH9PLPtlQghvtitDEZmJ5j+EEB5u4/4rA38MIfxPghxbrj916ZcF/hJC+GcbaXcA3isizwHPhhC+m5CPUW3WoWnAP9H28Nc20s8GlgNGhRDuaCP9ZsDPQgh/alOGU4A3UvSIiEwOIfwXIMCg8YwZFj0BEdkNuAu4TkRuAG3tLV5jrogcbmmXiEjTZSMiewPfRBva6KyitHj/OcAtwMeBh0RkRovKfh5wI3AQcDWwjR1vOh8isgdwFfBp4EQR2aeFtLsA54jImu08f3T/RSKyM6gMW8z/XOAa4F3ATSIyv8X77w18GzjMXjztyHF7YH8RmdRKuij9rsBtwKUicm6LaecCX0LXvEwB5orIii1eY28RuR3+5UrdUqfQZHg9cC1wiohs2GL63YArga2AC0Xkky2m3wV4FPhWOzIUkT2BpyzES8t10K4xD7haRG4GDhGR5VtJP6CEEIb0BmwPPI8quFGosE9v8RpbA39Ce9cfj46PaCJtN/A0GgzuLOC9aM8EzO21iWtshsYM2sb2T0AV16gm87Ax8BwamyhL/1Cz97c0W9g11gfGAMcBn20h7V/RRn4+sGYrz2/nrgO8hr44PwPsFP3W73WADYBngbfb/s6o4ly6yTJcEfgR8HXgTDSe0zItynFrq0N3A/sBk1qshzsDPwX2RIMN3gwc0GTaLrvvrlG9/CGwYwv338Tq4QvAQ9HxUU2m39bq0AxgDeDLwJEt1oEnIxnOQBdnLtdkHVja6s4+6IvvXmBZ+62ZOrAGsBg4HXgEWNBKHYxk+DwwGzgU+AqwQSv1YCC3Id3Dt97H6sApIYQfBh1+fgJYqsVLrYkq6lWAj4rIJ6Dpnv5/Ae8DjgReRpXfu0VkbAghiIaF7o/J2TNYb+J+YGII4S3LQ389jD8CZ4QQHrV8nwf8J9pQmmUUcEUI4dmg0UzvArYRkalNlMFfgaOAc+z/o9vo6f8ROBA4Hvg7sKuI7GTP00w5jkFf9D+y/P6CmlmgmTL8b+CDwBFog50BHCAiy9n9+ywD+30ysD/ayPe0Z2iqpy8i44F3oB2O24HHgMeBlZpJH0J4AzgNeFJERoYQfoO+9NdoJr0xCq2HawO/E5FH7NrN9vRXBC4NITwTQvglcCs6yhjbwoj5LJPhSDSoWBeqtPutS0HNR1cBd4QQPoS2x5tNhs2Mll9C69CpqB45Mu7p95fY8jwHOC+E8EgI4WvAX4CDm7h3OXT6jZO6oUq6K9p/O1rRm+5lo3a2Fez/tYHfAydFv0/sJ/04+zsGVRoXAQfasWlN3H8ssHK0PwH4QfQMy/STfgQwIfp/BDqsXzfLA7BUE/nIekOjganA96Lrrt5P2iyvm6I9pC8Ab7NjS/eXf/s7PpLpKcDngZ3t2OQm8r9C3f5d0TN1N5F+TPT/gfYMR9j+ek2kXyZ6hoPR0cJ7gOWarMtrAStndRY1Td3QTP1tcPws4Ez7fw42guznWstF/38TeDTa77MOxOnRl8emwN3Rb8s3kX75uv0bs7aR1edWygN9Adxr/+8O7NVXHaw79g7giagOrN+fLK0OTI3q9Hzg4uj3sc08w0BtQ7aHn/UYQgivBO3dZMf+CCwJ2it5H2qP7pOg/M4m215Ah6bHicgHzLZ+bF89nBDC/9rkzt/RCvZTYG0RuQ54WEQm5+R/iyj9/4UQXrXjAowHVrVnOBy4UURGN+rhhBCWhBD+YrtZPv8MvC4i7wYuoMEEfXzNEMKf7O8/0HL8P+CvZgv/jIgs00fat+zv48Cdlv4gETkLWJhXflkZBOt9hRD+Zn9fQZXlH4EtRORy4PvWC25ICOF3Wb5E4zWtDIwVkUOB20VkYl+9xGATzvb/tcCPgVXNFvuQiKyQ8wzjovRvRs/wddSkMBfYVEQ+JiJn9pP+xRDCq8E0A/C/wPJ23kGioUny8t2j9ykio+3f14CXbaR0BvDbRs8eXeu/o7a1D/CqiHzf2tLpIjKhv/T29y3gVXS0loVDPz5+3gbp/9POz+TUBSxl6W/Ja0s51whZGYQQDgGeEZFXgYvR0Vteml4jgBDCA8AxwMEicgtwGRoWpq97vxlC+H10vVeBSfZM+wMHNjnqHxg6+bZpZ0MrQMNeO6osFwGHoMPiDVu8fvZmnojaY98A1s85T/L+j47dCvwK2KjBfe6gZm+V+usB/45Wth818wz1eUBtmFcAD+elB1aqT1uXh7Fo7+oLaC9n/ei3PmVg56yI2pRfBzZppgwanHO3lWOjazSUA3ADOkp4EJjRX9k1kOM37P697LDo/NHxOfkYFf0/B3gKVbabNJl+pP1dF31Z74aO2NaLzplC3RxB9Bxj7e+eqFnkwbz8t9AmfoGaCFttS11Whz6Beo41HCU1kgHqgPA1e4b1637rqwzi0dr+qOm1Vzvurx3ZsQvQUX9uW+7nejtaO9wfnWNaq105FLF17MZtVrzdUGU+OTqWNY6p6KTN8sBb6MTTOk1eN1PyU6Nj26Phm+NGtjmwB7Bpli5KuwHaKwd4G/C7Rg3E0n0NmxSKrvE2aiaBZ4CfUzeMtTwcCPxb9OxZ+nWBbe3/+1C7dK8Khirja4D3Rccyk8z6WbmhPdwXgX9rUgYrUDMB7Q+82aiR91MGS9n/s9CRyoycMmhGDrcAv8wpw62tDOfm1IGZmGJAQ3j/gsYv7ZMwB4G6PEyMznlno3LoLz06t7QEnciMX7i7oC/Lj2BmFmqKbgvUpDgGNQktaVAH1gOmE5nBovtPB6bY/9uibaleBv2lXwFYFm1DTwFrtyiDGfb/pSaDderS91cGF6Kdv6moaWuDuvT9taNtovMW16dvQpdkJuL10JfND/PqQNlbx5V40xnV2e+nMO8NevaitkPt9tOAcejQbe2ca4yJK0bdb28HrkNj9gtwWFzJgF2t4p2DTsRsHP22Barg3x4dm5Jzj+Wo2Th3RCd3st+2QieZNkEb/6n1DRU1DzyP9lzvoafdf3u0BzHb9rclUtR11+lC3Te/ChwUHd8W9XLKrnFo3XM2I4PVbH96XgVvsgw2tP3R2fVakMPrwNa2/z5sHiE6Zxf0JXYyqoh3jX7bzNJvHx1bNucZMu+dTwOfi+sUavd9zJ5zjOV3vTbST0Zffg/S84W7Ozri2pXevduNUOUcK9EVcvI/F/UsuwX1aplETVHtgI6qspfe5sD0NtKvZ/tnU/fCbFIGO9j+3pjXV4tlMCc6NjYn//21o81sfwSwYpu6ZAWrB48wSDx1Op6BpjKpgn0tqkTTUYW4LDqs+wbwruj88TnXWMcKfqVYUKhyXw34DbBHg/vPRYfU77D909Ge7lq2fxKwS6MKYMe3BS6x6xyD9pK/i3oUjURNJ/Oi80fUpd8RNS1sbPs3AIejvdDlUDfCffspx2nUJpjHoROKVwOH2bErgb1TZYD1mFLKoEFDaloODe4/B/V82cr2j0N7eVnjPhpzY+xDjuuj9vDVgQ8Bn7bjo6187wHe3UceWkpPNGGPulo+BmxXd82j0JHEstR6prlOC8A8VFnOQB0U7kDrf/YS+iGwXx/5T03ftAwapG+lDPImYptuRzSuxy3pEiLzUqe3jmeg3wxqIR6I2tBWQntNj1KzfY6PCn50P9f6qlXIvDd2d3zP6N4jUKX0PTu2MjqZeBWqAA+hpkR72OLrKvkTqL/8rujM/bWoff0T9kxjGl3DKtH5lmY86sXzMupnfgNwbvZMefe34zPRtQb3WgXfzq61HzpsjivoSHralFuRQaNG0nYZpMohkufXgcft/y50fuYrqH39o/3J0X5bE31JHm3lfhbaw52IznusmZeuiPRW9tfWPdM5Jov7gPf3c+/lUZffr0VyexW4ycoxHhnk1cOW0te34yJkkFIGFNCO2tUlg2XreAb6KdTtgd3s//mo98uvgPfWCXwaDRZWmFDjXlJWOTLBjra/8zBTQHRu5tI3Hvg+Oky7H/iQHd+RyITQ4P67osPXTXN+ewdqbz0Zs303SH+W3etMdALpaeAD9vsOVoEbutyhPeilLe+voAr/WeBzaK/+I2hP7V05aYuQQWoZJMmByCxjz/kddDL8Xy6XaK+s1/NH6cZTm1tYCx2d/NzSZT7z99h1b6XOlTYlPbAh+qJYwWS/ZfTb0SbfWeicSK4LLzWb8l7oRPZZqAnuCHS08V7Uu6qRGTA1fZIMUsuAYtpR27pksGwdz0AfhbuLNYjYLn4oqrA2io5l/s55ttbdUF/ye1Gvl8zkcFadoI5AFdIadRXkB6gr1vtRt8ZbiFYg2nmXAO/s4xleRhvyptHxrGKMQO3ilxOt8K1L/wQ1m/SmqHK8jci/H+2dHNIgD3OAz9v/y1gjPQU1w8xEeyp3oJN79xD5zBckg9QySJIDsJM94z7Rsa8CP6k777PAwQ3KcHfUJ/0hauaCbtQT6otWz7pRE8fW2KRxanp0pLUMOm+ypR07FbX9/1vdPd6PmtXyzJkroi/J7CU9D31x3hGdMxntqb9tANK3LYMiyoBi2lHbumQwbR3PQIPC3RX1TMgEvBK1ib6DURevDUyQi8l3m9wV9dCYiy7kOQFVGifb7+eY8D5mAooV2BzUQ2U/1NPhVrRnM96Ee4mddwDa4+0lXFSJPQe8GzWHXE5kX6bmGTAS7UXXLxraFQ1AdZntZz3pDYGF6KrK1VBvlSfJMQVYHp6l5yTkclZm50bHlrWyXKtgGaSWQZIc0Eb6lKXZpu6324Fv2v/7oZN4ed4sc9He93bAicCvqS1oW8Oe6WQaLApKSU9tIvReavMU26Kjg9MxOzbqYPAIDbzSqE3SX4XZ1y1flwDHRWW1GFilyPSpMkgtA4ppR23rksG2dTwDDSrol4Bn7P+pqNJ6b/T7fNRdL8/lTtAJsC9RN4lpgruM2sTgZagpYMMo7VTUjepoOzYG7U2cYPsTUM+JR62C5XmirGr3ymKCTEcnlRZhK0fteCN7905W+Q5De9Mn1P2+KdqzuNMaSZ6ynQf8DPMOQHuQR9n/k9HJzwsGQgapZVCEHFDXuueJRid2fG9qJqKbrYH+uIEcV7PrnxEdOwk1X2S25m7UxfWT9PYGaTs92tvPJjbvoKc30nZ2nRfQQGW5XiDkT9Jfi8XnQUceF6GK9z/q61EB6ZNkkFoGJLYjEnTJYN06noGGGdPh1Q/QSb0P5Pz+7voKUvf7RdjwjJ6LME4GvhHtLx/9n/k/H4kquM1t/1K0N5F5Pkyw/OUp2rGoffJMdCIu61GsTk3h7dRHvpdGex2zbH9DdPHMcXXnzUR7KI16dQuBl+3/8Wgv86jo98nWWBoGSGtXBgWUQRFy2Ba43v7P7n8ROnH9eSwcB9pLbbRWYAIavfRszFyEmmYesHwdgs5BLE3+5F1b6VEz1+fRycxtUFt3r4Vn6GTvBHLCTqDK7M/kT9JfhnkCoZ4td9Lbzz4pfaoMUsuAgtpRu7pksG4dz0BUWMvRu4f0JeC5aL/ZqH2Cvrm/Eh3LeiproLa7MXVp1kRjr2TK5Qi0h3xdfD59eAKhftOT0YmlW6zCTYl+Xx11xbueyMwS/b4z6q64sO74jAaVtVdesIiZ9v+ZqO3yKWB+ThlNoKcNM1kGBZRBkhywOQh0Au/G6PhqwLHoyOMqbDjeRF2ahAZ0OxftyX7b8vgh4Dy0Z9owzk+76U0251i6F1APlNNRm/+paO/2PQ3u2Y3OddyDejA1mqSfG5dZgekLkUG7ZUAB7ShFlwzmreMZsIKbjbp3LaK3ieZmayRZr6+Ru9xWqC33QBPEWFTZfaHuvCOAb5ETxAidNLqV2grO+eiCqmyira+KMQftCZ+ATjJ1o4rrNHoqvOnAB4hCG0Tpn7aKuhj4bvy8VllfAz7ZT1neCTwV7Z+NDmfHRccORBV5bEIoQgZJZZAqB9TT4yR09eRyqNfHh7M8UxsZfBBVtnl14O2oYptAzatmMuoJ8gDRxKP9Vv+CbDs9dYH27BqnoqOzy9HR0aloj/TL5E+QTkKV0G6o58pDaE80b5L+XuqC6hWQPkkGqWVAAe2IAnTJYN06noGokv0IVSxPo/bMA6Pfr0RtobkRH61y/gL4FKpgbkZXAE4xQV2F9iyORe2FM6K04+qudZpV6JlRxXyGuoUedWl2RXtA9ZNSq5Gv8EbVnbeJ5f+d0bEbcirzpnbeZPrw70V7wz+M9j9rDXesNYgn6T2ETpVBahkUIYctULe8E1FT0kz0JRabsubbc/RyH0TnCa5FldmFaM8uMwkshSrtz9PYm6ft9CaXO4g8dKJ0Z6MvkdiG3eilOxH1Zz/b9qeYvPucpC8wfdsySC0DCmhHJOiSobB1PAORQC9EZ8rXRIdkv0Zte5vYOeeQE+LWzn+eWu9vPDpZ9G1UaYyxyncmOjSMY5KsZfetXzp+KtpT7rb9j6GTUvVKSdCh7xeouWllXgWZ6aEbNW2cR46ND/VQmYH2qg7CvBxQP/PPWp5XiI7n9UqnELmiob3rt4B7omNnoguVniF/krUtGRRUBqlyWJpaD24mauv/lJXbxtYwb0ft58/QdyC1DdGe+AloL/FxdDVoNrH/cVQB5IbMbic9NbfBraIyzcpwktWRs9GX7hZxGUfX2IKaolyLaN4FXTD1GH1P0qemT5JBahlQTDtqW5cMla1zN9ZKMY9asK2tUQUzxSrIK6j3wo3YxE+D62wM3JZzfH1LG0eFjCuIUFM051IX6xtdkfejaL/h14vQ2D0HZxWv7h5dVtGuJ4rbb7+vgyqxpdGYJdeg9tIvobbdI9HVpd9GTRzj6N3Qu9EeVTYpuJJV8sPR3k385aJP0XN0U4gMEssgSQ7oQrXcSb8AAB+ASURBVJqnUAWzLqqcVrL9j6K9uGXst83In1xdy8ox86U+g9qk5Cmot9BDqElhY3pPELadHh0Z/QW4LzqW9Wy3REdbK9oznE5+bJzJ6OjscXSCc4I966+ohYqYjK6pOGsA0ifJILUMKKAdpeiSobR15qYq4J+YUGZGx49GbYQvUVvdOYkouFF0btabyMLw1g/bJqJLn/fOSbOuVYqx6Ft9IbrybvW6a1xHzX0sb5n3SFRhnUNk36NnULFPWl7y3A+zyJPHob2JLaxiLaZnFMJ/RS+sSz/Z/h6DTmQdjA6VPxSdcwvw/EDIILUMipADajP/LfA/aC/scdQN7yz0ZXFS/fXq0s9B1wpcjfZqV0fDCn8f9bt+Dh31jLVz6pVV2+lR5fRTdA3CnVjIAvttJdTTabf6+tvgOT5h9zoCfeHsgXpRXUitVzuJulFUEelTZFBEGZDejtrWJUNtK/+GOiHyIjlLmK2SvYLFdaGxn/r2VkE/iPYcbwY+TO/e73l5lQXtjX3VtjGoZ8lCOz/7StNBaK8nb5n2tLr9qWhv6My64wehPbuuvOewc+agbl8noD2rDVA78KHkRPyM0s1FVzZmqx+PQ/3SL8059zp6euMUIYO2y6AIOaA+1vvY/zPRUc3HUN/xk9DRxK9Re/pZ5HgX2TWeRkc2S6G9x3+3365GPwCzT97zp6a3NIdRm5gei3qhXJlXxuQrug3pWb+/bmW3IdqjvQZdebptgzykpk+SQRFlUEA7akuXDNWtvBvVGvkHqUUIzI7FppYL0bd1o7gsc1Ab4NFoz+B4dJj4JOoulsXRPgh154vDJcRhA6ai/sRXW0VbA7UZv4DaCp8h38e8fmIp8zr4N3QR0mVoj/sYdBKz3la5FeZyGB2ba5X1ONT/eFvUFnk4+SOD3dEhdI9l/FZRv2zX6xWXpmAZpJRBkhzQScOtUEWSvZhmWx2IY/zvgDbmPDmOQk1e/x4d2xRzv0NNXV+PfqtXAG2nR0dXz1FzPR0VXfO+unSNJmfHox4rvwU+ZsemoS+dNVEz2iXoNxEezcl/avokGaSWAcW0o7Z1yVDdyr+hvtGz2C5ZPJUsGuIW1lDOJWdSDLWl/Sfme4sqh9dRG96W6Bv9fnRy6Gl62qvfgSrJI7FY2agHydmWLvtK0G5WGfOGrn1NLC1l2yfR3szZ9ZXczvsw6gWwWU5lvYzahNS25JuyJqH+0bkhYK3SXo6uZuwVV6UAGSSVQaoc0GH+vVYO89B46nvab1uh5oTjm6yLK6M23bNs/3PA1fb/OFQJnFRkepPzYmohnpen54R75v9+Y5PPsAG6yvTzaCyZo6j1uidYPeorAmfL6VNlUEQZkN6O2tYlQ3kr/4aqiF6gZn8eHf12FBpzY2qDtBuh3iBnUPvKzVVRxVkanSBakzpbHfqW/wvaM70b7VF+Gn3LX4HawHMVpKXvb2LpGWsIvXrMOdc6ElV6W9Qd/ywW86NBumnosPNB+vhUGqr0L6HBx8PblUERZVCAHFZAPTWyl8U+9FQ4s9FRxgcbpN8UHcZnnkerWF4eQINjxXMPG5Pv0tdWetRH/UUshK+lvZ/ah9qzshyNKppe6xTQF+GV6BzB0XaPCaib4JfREdX9mDto0elTZVBEGaS2o1RdMpS3ztxUldGz9PRWOMiOrdpP2s1Qt6jPoj3Z2+ljpRvqqpVFyfuANc7N0Qmpj6G9m2+hQ9PzGlyj6Yklar3dOB77Nnbvz6Bmj6XRCdYniHoo6FL7heQryt2wzzZaY8s+4PCv2PVWOTOb/nJFyqCAMkiWQ3Stj6AeI9nS/L1QhbN7VEfyRmhZmObL0Q+EZytFp6K26oWoomk0b9F2etQEMhm1iy9A7d8/Ippgb6Ld7IG6De6OLgw6BZ2Uz2LbrIJ6pizB2kWdDJLSp8ogtQwooB2l6JLhsA3sxXsPBWN3vQtQu9jlqM34Bfrwa62ruJujtrqXqE3uNVp9eTw6eZSZIE5CJ2ayId80dMLpCvIX47Q0sZSTfi9UUX4CnZy82/K0Bqpgf4MGpTqaviN/vkhtEc/5qLtZfRjY7KMiS0XHkmWQWgapckBfKPUhh7+MKo2sR7g7qqjmNrj/NqgJIOvBHYS+pLIvNa2CKr8vku/J0XZ6k1/20Y51rLyfpS6OkT1DbpgGVLndnN3fjk1Ce9c30TMi6mH18iggfZIMUsuAAtpRqi4ZDtvAXVhtaT9H3dSyHmj8oewxaECmd6GKquGECPkTixuidt8zyV/xNwuL1436Al9BTdl8HPXJ3Yw+YsOQPrG0plW+OPTyPKtgx9j+/mhv+4vk2/x3QSfWvod959OO34j2irdBe8/z0cmmeN4iWQYFlEGSHFD7/rWospofHT8Ms5fXPW+vIFjoPMOn0EU/60X5/wbqQ559Xm85dE3D1KLSm/yeoqdCnYpOin+UWqjk/VEzSK693WR1D3WfYLR7ngkc3k97bDt9qgxSy4AC2lGKLhlO28BcVIX+cFS5Yq+M2WhPcqt+rjEbVURZeN14cnAGqsRmor3UPJcvsQr+Ptv/lFWwTNkcj/rWZr3Wev/uIiaW1kJ7fULPXvceqA11Wj9lsD3aq3mnlcUVwP7R72ehi1vuRoejG9TlP1UGRZRB23JAXxa7oi+E3dCX1wWo7XYEOsr5cH26BvkYZ+V1ETpReT4aU+Ua1A31BhqETGg3veX9KWofhe/Gon2iJokrUI+SU1BPmNyY+lE5XkRtQnUktfZwGNpL78ttsa30qTIoogxIb0dJumQ4bcVfUCe1XqFmF5yG2ulmoB4cl9Jg2B1dI/vS0pnoBF/8ObPN0W+rbhXdL/bxjsMdv8vut7JVzjPQGfzsox5HAavl3D9pYgm1U45DF3o8Qs3zJJ7Qu46+PRmmox4Gs6M8fABVlvFHrkejw/VYERchg9QyKEIO+1n5ZSEbVrVj30EV1CJ0pNNocrq7rszHobbde9CXYTZaWQd9udXbnFPTfx74mf3fhYYniBfFrY2auZ4g35TWjfY+10bdKOegL5lZded9ELi46PQFyaDtMqCYdtS2LhmOW7EX08Y8C53t3tcK9D5s2GXnjLe/fUVcfIzaBN/JaA93fdv/KNEHNOrSrgP8A7XjrYfamq8FTrPfR6I+3tdR58MbXSN1YmmOpT3U9q+nZ1jf7O85NA5vOwL9jNpFaJiDrDeyIuo6t4jGC3qKkEFqGSTLIbrW3mhM/v3rjh+PmlT+k5wJatT75NdW9jOoeWKMQ0cZl6LKsFEwuKT00XWuQl8Oj1L3TQH0RT2R/HAP89De853oCOg7qGnjAPRlPh/t4R5CjttgavoiZJBSBhTTjtrWJcN1K+5CWsGyz4jthdrTfoUt8InOm0kf4UTRodsD9v+KaBySr6ER944k56v29LRH3oT2Hi9Gh6pTLW0W22QkGgApz+UtdWJpd9QHeVtq3gtTUK+au9BYIOPQHu8z5Ie33RZV6suhZoLziPyIrUzeZw1g96JlkFIGRciB2rqA2FX0nehLaz96mqZG0OCjE9Tiw7yAut/dEN17NNr4v07U4ysiPTrBvS/Rx7iJernRsfnoyKjXCwMdLfwYm7NBR2jHWl7WRF/AZ6NzCtdQ95WlAtInySC1DCigHbWrS4b7VsxFVEk8ivnl2rEtTUALsEaNulA9TwM/+6gx3Y8qjB9ivrzU4rX3eiPTczVt9u3UyVaZjrTG+U36nhhOnViagi5GyUwwWa98JNpjvgb11b4NHZ5uVJc+O/8z2Me8Ld2NqL04VvorW1nGwZySZVBAGSTJAbXVXpfljZ5D931QhfMu+vDTr7vefugoY7rVnzetPOajCuMT5AQjazc92qN8Hh2ZPUHPTxt+FVu/YM/wGPmT9GsBf6U255G9REeiPepbqPVuR9D7Qz5FpG9bBqllQGI7StUlw31Lv4CaDP5GbUJkGrXVa3ui7n6Hoz2MB8m3Vc5CvU0yW9poq5g/rjvvAiL7tR2bCLxsFSxb6n8ZGsRpNOr+dwXqLnYYAzextAw6ebo6jf24V0RtoL3shNTsk58h+jgDPZV+rOBj98oiZJBUBgXJYQ3L61XUenajqDX6PdGe354Nync2cFi0v7WdPwlVJL9DlfQD6IKv+mBubae3c1+Iym8m2ptcNjrnSnQ152LylX0Wavosu35mesiU9vpoz331Bs+flD5VBgWVQWo7aluXVGFLv4Aqm2+hE4orora6OJbGPLTH8DPyFc0c++0Kqyzzo0p2P+brjfYqf0r+V+WnoyaOS9FofxPQicGdrBFMR80buV4QJE6u2TnLo4uJ1o7ynzW0VVBbaSMzylp2/aVQpVof9mASaitdRL69N0kGBZZBkhzsGtPQidFrqCmcrBxWQH2uV89JNxd1S/04Pd33TrA8/5ZISdE7rn/b6a3MjyX6kDe1tQrvoacr43HkK7p1rezGo6OHk9GwDfUv1tvJX1SWlD5VBkWUQQHtKFmXDPct/QL6Bt0cVUh/BI6043HvZxvyV2/ugPp4Z4tv9kE/WrxUdO370DC+jzRoKJm3yDKo3fAq1PZ6FBYUqsnnaGdiaXnUvJJ9w/PjqKmj3qZ5CNqzaTRBuAZq674cXfWXDT1HURt+r2RlkxcPvW0ZpJZB0XKwa0xHFU5sWjgKnUTNmzvZ3Bp6LzdT1GZ9PzUvk16miJT0qL35B6hyOwZdV7AJte+wftmufQ8533ClZ+TQK63cxtFTaWe+/4dZmU4uKn0RMiigDJLbEQXokips7SWyHhA1t7Sx1mhuJ2rc9LNiDTUPPJAJFu0l3IYqp+3s2EirMH2tnIvtjKPQIFYvoOaDdzVIkzqxNA+1NT5g23loz/hT6Kq9TVEPlQPRHmKfwZdQpX86GlN8CTohmFXOO9Al72Oi85NlkFoGqXJAe6VrRvuxD/4a6MTil9DQC89j8WtyrvNO4NSsvtjfeGHNIvr+iE5b6aktSts0OnY82st9MDo2Ah1B5DkK1EcOvRT1aIp76lejbeWR+npUQPokGaSWAQW1IwrSJcN9az2BFuSfrTA/Tc/gUJuhw8oz+rnG1tTikHzBKuiKqLJ52a7xuv2tX8yyFTrMfjvmCkbNvrgBtRWhu6I95jx7e+rE0hxLtzNqttjZ8n4b2ls5DnWrvB11i9uwr/KIrru6NbCHUVfAtazCziJaAViQDFLLIEkO6CKeR+ueK+utrocGt1oDVbZ/pu/Juf1Rk1b8ofYsL+ui9t7raRAEq530Vn4vo2agNequdzgaT34jGnwK0c5rFDn0LLvfONTM9zl0krHeTz01fZIMUsuAAtoRCbqkilvrCXQS8U4rwAXoEG8+tTgvM9El2Kf0IeSn6bkA4ny0R3BfdGx14L30jPeefSj7cvSjGodGv22F2lm3jo716t2SOLFkFfi/ib5+Y8fXRHsnZ9r+RGtwvRak0Hev6m2o0v8Kjc0nqTJILYMkOVgdeAjYNSqr7ItWW9h9s6H5qvQRMTGSyTeIlvRTU1yfs7Lp9X2AdtOj4SgeQ/3Y55ssZtdd81j0xZ3r9mnn9Bc59GI0JMIY8ico206fKoPUMqCYdtS2Lqnq1vyJqhRmo0OzbVBFsLwJ7kZ0EuSTqO1uC/InF3dH3aN2sv2VqLn6nYn6Oq9IzsIg1FzxP9TiluyPepxMtP33AfPs/75m99uaWKKmALZEFV0vlz50gcqP6HudQbO9qkvQnlUcDbMIGSRNrqXKAe39L6GmaNZEl8dvjNpZP0TkWtpC/bwEVTrrR3k5EPXTnlZUetR19SFqkUFXQG3bd9A7TO8HqZsctuOtRA49dwDSJ8kgpQworh21rUuqvDXbmOagCuIQ1EVvJNrzmWGV5zfWOO5Eh1J5E2sTrJKdYfvTUYX1zuici9CFFXnB0Da33z4eHbsLtfVtT+3rNI1Wj6ZOLMUToJugbl0L6TnMn4yuaFy2j3Jstle1Oj2DcBUhg6QyKEgO2Srir6KjmXuBjzYo5z5jw+ScfxE66nkQ7ek9R2Obb8vprfyepPeCt+WtPO+g7oMcDe6dGsE1NX3bMkgtA4ppR0m6pMpb/yfoarcX64WI9v5+ga7kzD58sDQwKecas024OwNvoL3A71KLdBdPkJ2KfTS57hqZJ8rV6ETUZ9Hh3GWoze9nNFZSqRNLO6MKYCG1RVEboUPPhdRc196P9qzyhp9t96oKkkHyBGOKHNAX1iXRPb5m5XGcHcsmn7emsZLuKwjWWqjb3hqoUtqJ3iF+206P9hbvozY3MQZVPKvY/yPRRVoPEH0Uvu7+qZFDU9MnySC1DCimHSXrkipv/Z+gSiErzH/5xNr+DcBFcWXJST/CKsHFtr8P8BY1n9isp7UvvRdV5XmibGGV4bfReRNQu3jeyCB1Ymk31EXsYPSjD7eidsll0VgqF6DD0o+gvYxGyiqlV5Uqg9QySJID6jFyB6pcLsieEXUhvImaj/ehVrZ5L/z+gmD9AVPkDZ4hNf1yqJ18A9SmfBoasvoBdL5jWdsWkBMILnrmlAiuKZFHi5BB22VAAe2IBF3im5VhHxU8K7yLqU2g9BgGoz28m2hgM4+utR06wbOG7e8B/ImarfcgdPVes54os9BJthObvG+7E0uTrDLvUnf8WuBh+3+mNZrf0nhhWVu9qiJkUEAZtC0Heiqc2ajZ4TXMxRFVOF+zaxxK41XATQfBIscMlJo+yusJ6MvxZcv3+9EX31XUvvSUN2+RFDk0JX1RMkgpAwpoRym6xLeo/Po9QRc0fA8bolkly4bBC9HeUp4HQX18+vOtUmWLK/ZGe1VfRhe21E8ONvJEySYLZ6JxWRp5ohQxuZYt886+ajQ2+u02agukZpHfI0ruVSXKoIgyaFsORKsxqc0brIs2/puj3262Z2i0+rLpIFgDkT66zgR0svHddXXhK8BBDdIkRQ4tIH0hMkgsg9R2lKRLfIvKrt8T1CZ8GhqGdGZ0/EB0UiQv4uNGVrG2io6tgw774h7iPiao+MMdrXqi5C1mKWRyzdJ8G1gQ7WfK4WSimDd1aQrrVSXIIHVyLUkOlvYfaFyWeaiN9wjgUvv9UeC66PyV+8hLUhCs1PT9XHs/tMdZH0guKXJoavqiZdBOGaS2I/u9bV3iW055NinQVdDh/P2Yj6w1+F6Fi9qnT0R9yV9E3cQ2sd+uBL5cd378BZsiPFFSJ5bWtntndtHD7Zm3qzvvE+hwuldoVQruVbUhg9QyKEIOW6Iujfda2m/Zte7GFtCgk81XNnjepCBYqembkMdKqCnsWfLtzamRQ4uIAJskg5QyoJh21LYu8a2BzFoQ7njUznwa2kvIc/eah/agsmiJM9Gh3vWom9QM1HSxSZQm68lsS6Iniv2WMrG0F7pyMQtrcD7q3XK+KYZj0GXf70cnAPNW8Q5Yr6oZGRRQBslyiGQ6G31B7YUqhjNRG+0J0bl5gcCSgmClpm9BFvPIH10lRQ5NTV+EDFLKgGLaUdu6xLc+ZFbYhVSgD1Nb5jzJ/k5AV+rdjNrrlgDH5qRP8kSJhU57E0tbokpiY9ufaOefgdrQD0SHpbeiyrORN86A9qqalEXKBGPbcrA6cBbqjbQnOtLYDHU/3RcNMbw26hUzokHek4JgpaYvUAapEVzbSl+EDBKfO7kdkahLfOtDPgUJOfMxz76ws6YJs/5LOhuivZO8FazJ3kDRNduZWHofsND+zyaDlrNGd1503mgaf/xhwHtVLcikpTJIlYMpl+dRe+5pqKnnm6gpazN0wVdu2dddJykIVmr6gso+KXJou+mLkkHisye1IxJ0iW9NyCdRuJlCmIi+dS9FV/nd20zFrrtWW54oLVy/0eTaZPt7FHBFdDxrdMujAZ42orEXSEd7VallkCoH1LZ8H5G7JxrE6yj0BTEZ2NHK8d155UhiEKzU9ANQ1i1HcE1JX4QMEp83qR1RoC7xrQ85JQo57jmOQz+Y8E/gKDuWKYqt6OOjF3ZOy54oTeaxr4mlzK4o6Mz/80QfB0cnXAV1eWvkMtnxXlVKGRQhB9Rr5Edobyw2AXWjyndH29+R/I+XJAXBSk1fQPmmRg4tIgJskgwSn7+IdlSYLvGtD1klCHkX1OvhtEy4piyupuekZPbRhX6XONOCJ0oL+Ww0sTSH3t9vPcQqZRwf/l3oZ+HyPjzS0V5VahmkygH1osjsq1+ntsIz7p2ei01YN7hXUhCs1PQFlG1q5NDU9MkySHz+ItpR4brEtwbyShDyo8CH0Y92XI6tbEOHZFdb5ZuP+pj32ausu3ZTnihJD60ue7+k9hWjbnTFXje6Uu8Fe4ZL0d5Kbix2OtirGvCK0Y8cULvsBaipYQxqT32Cuvgn6MjipAb3SA2o19EgWqRHDk1NnyyDxOdPbkcMoC7xLUdmbQh5sjWyPWx/VXRSKR5Oj0Hf2H9lEE6qWCW7A3VB3ARdih1/v3UNNObHfuT4OdPhXlWHyy6ztQoaZO3sTJlYw37cynU91HzydF4dIDEIVmr6gsoiNXJoW+mLkkEBz5/ajoa8LhlqW7uCnocOQ7Ol0nehb98LUbe+5UyYfX64oqMPrpXwOrTncVJ0vD+3z472qjq9UYv9k9lU97fn/6jtH4/6iN+Dut/lfaUoKQhWavoCy6LtCK4p6YuQQYFl0FY7is4b8rpkKG0pgp6LLtC52CrVO9GFPD9GXf+W6fTDNch3bHqZg3oE7E3dB5MbpaPDvaoOl90U9Duj2UetV0ZNWl9CJ3lPxibf0IVdvcLbRtfajoQgWKnpE8ogNXJoavrCZJBYDm21owbXGpK6ZChuqULfCR2SrRAdG0GDb4d27CF7u2LGw/290R7KwUQfHMm5xqDpVXW4LPdAF5bNQF3mPmzHt0MneM+j8YcrUgPqdTSIFokRXFPTFyGDxOdPbkd9XHtI6JKhvhVRCeaiKxtbFnIpD6j5+znqkpf10jPlPQNd3DIPXfl3ADl+8gySXtVg2dAe3ZJYOaGLmXZFPWPyIncmBcFKTV/Qc6dGcE1KnyqDxGdPbkdN3mPQ6pLhsBVVGfZCh88dW1TUIF9z0SXambfM0tFvs1FXwyy40570HbGxI72qwbqhk6U/rX9mcgJYkRgEKzV9Ac+aGjk0OQJsqgwSn7+wdtTEvQalLhkuW5GVYkKnH6YuP5sCrwAH2P40tFc+A1gKtbvPbfGapfaqBvtmiuAFcqJlRuckBcFKTV/AMyZFDk1NX4QMEp+/8HbUxD0HlS4ZTlvHMzAgD6U9qVmoi9e+qCfEfZjLnp3zr4U4rSgISupVDZUt7pHVlyPpAfU6GkSLxMihqemLkEHi8w9YO/KtM1vHM1D4A2mP8DL7fy80ouCvgE/XnTeTaDl3i/cY0F7VUNvyemQkBsFKTV/Qc6V+S7iQCLDtyiDxegPejnwrfxvFMEJEdkW9H04FCCHcJiK/R4eer4vISiGE10TkYFRJbAv8vtX7hBC+LSJjgO+JyCw9pLW/ioQQ/pL9LyJiZfESumDm3SLyS/Rj298JIfykLu1P0FDFhaQvgigP01FXT4B/hhCCiIwMIfzT7rmJ7b9VZPp2iGWQSlntyCmfEZ3OQFGIyOaoh8DxIYTviMg0EXlPCOFhO74esJuIHIt6SOwbQmi7koYQbgPeEUJYUmVln8MYgBDCn1Hb9ATUfHFrCOFcERkBICJbici6A5A+mUietwCzRWSmKesRQPbbRNQ+P7no9J2k7HbklMuwUfjGfcB6IrIiOqk3HiCEcDvqH78j8CHgiBDCs6k3K7JXNRwQkV2Ab4jIaSKyTwjhf9E4PNehsfkJISwRkcPQ0BNvFpl+AHgUXfW5vyntJXb/A1GT08YhhDcGMH2nKLUdOeUhw6VzKiKjUbe249Ge04khhEujITQisg3wqxDCyx3M6rBEROagwa++jgaVWxk4J4TwoohMRNcsBPTLX0cCR4YQnikq/QA+1yro91h3RFf2/g2dwNwvhPD0QKcvG29Hw5shrfBFZKMQwlMiMiqE8JaIjEV9m08BfhhCONfOGx1C+EdHMzuMEZHJ6GKnvUIId4jIqugHYS41UwA253EDGgp3sxDCc0WlL+H5xqOTkzsBr6Ex9n9WVvqBxttRdRiyCt+Gmy+iX5VajMbN/rn9thnaQ3kxhPDpzuWyOojIPHTV8ZYhhDdF5C50ZeljqK/5V1HXvbEhhNeKTu+0h7ejajGUbfh/Q+Ol/A/66bp7RWS+iMwKIfwHuohlfRE5pZOZrAohhLuAjwKLReRitG6dhy6VPwCVx1uNlHVqeqdtvB1ViCHXwxeRmWhY2R+jn4S7EB3mr4oOQTdA7cDfRj0+fh1CeL0zua0eIrITamdfKYTwOzs2Al2z8IeBTu80h7ejajKkevg2sXcp+kHwt6GfhPse+lm7JWgcks+gn4Y7EB2KeiUtkRDC99BFO/eJyFQ7tqRZZZ2a3ukfb0fVZcgsvBKRbdF42e+1oWZ2/HV0ef0I4CMhhNtF5Fb0259/7Exuq02oLUz7jpkGlpSZ3mmMt6NqM2RMOiJyPLpa8SIRGWX/B/vtBuD1EMIxmadBRzPrACAiE1LWKqSmd3rj7ajaDHqTjoiI/Tsd6LL//7VM3fZ/AqxS1DJ1pxhSlbUr++LwduTAEFD4Q3mZuuMMFrwdOTCEbPj0XKZOCGExwBBYpu44gwlvRxVmyNjwYegtU3ecwYi3o+oypBQ+DP5l6o4zFPB2VE2GnMJ3HMdx2mPQT9o6juM4xeAK33EcpyK4wnccx6kIrvAdx3Eqgit8x3GciuAK33EcpyK4wnccx6kIrvAdx3Eqwv8DbkK3QfJPYyQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create train dataset\n",
        "train_dataset = get_processed_frames(frames_train, x_train, y_train, emotion=True)\n",
        "# del frames_train, x_train, y_train, frames_pos_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYFii1Die-lu",
        "outputId": "b2050428-0bca-464b-f7a7-f00875bf71c7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The target sampling rate: 16000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4600/4600 [00:00<00:00, 63293.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Frames with mainly noise"
      ],
      "metadata": {
        "id": "Xg2DfRRrgQ0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all files without any noies\n",
        "if VAD_predictions == 'Perfect':\n",
        "    train_dataset_speech = [element for element in train_dataset if element['majority_label'] != 0]"
      ],
      "metadata": {
        "id": "okzIOwP1gnuv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all according to Wav2vec-VAD prediction\n",
        "if VAD_predictions == 'Wav2vec':\n",
        "    with open(f'{path_files}predictions_wav2vec_train.p', 'rb') as file_wav:\n",
        "        predictions = pickle.load(file_wav)\n",
        "    train_dataset_wav2vec = copy.deepcopy(train_dataset)\n",
        "    subset_wav2vec_train = [element for index, element in enumerate(train_dataset_wav2vec) if predictions[index] > 0.5]"
      ],
      "metadata": {
        "id": "tP1B7g6egTsO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if VAD_predictions == 'Unet':\n",
        "    with open(f'{path_files}predictions_unet_train.p', 'rb') as file_wav:\n",
        "        predictions = pickle.load(file_wav)\n",
        "    train_dataset_unet = copy.deepcopy(train_dataset)\n",
        "    subset_wav2vec_train = [element for index, element in enumerate(train_dataset_unet) if predictions[index] > 0.5]"
      ],
      "metadata": {
        "id": "dyK9bCpphylg"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if VAD_predictions == 'None':\n",
        "    train_dataset_speech = train_dataset"
      ],
      "metadata": {
        "id": "QJkQq-Eui_Mm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if VAD_predictions == 'Wav2vec' or VAD_predictions == 'Unet':\n",
        "    for index, element in enumerate(subset_wav2vec_train):\n",
        "        # as no 0 is in the remaining data, all values need to be reduced by one to start at 0 for the labels\n",
        "        if element['labels'] == 0:\n",
        "            # print(index)\n",
        "            subset_wav2vec_train[index]['labels'] = 1\n",
        "            subset_wav2vec_train[index]['majority_label'] = 1\n",
        "            # print(subset_wav2vec[index]['labels'])\n",
        "            # print(element['labels'])\n",
        "        subset_wav2vec_train[index]['majority_label'] = element['majority_label'] - 1\n",
        "        element['labels'] = element['labels'] - 1\n",
        "    # subset = [element for element in test_dataset if element['majority_label'] != 0]\n",
        "    # subset = test_dataset\n",
        "\n",
        "    train_dataset_speech = subset_wav2vec_train"
      ],
      "metadata": {
        "id": "VmSxeyQJhUiL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and validation\n",
        "labels = [element['majority_label'] for element in train_dataset_speech]\n",
        "train_dataset, eval_dataset = train_test_split(train_dataset_speech, test_size=validation_split, random_state=101, stratify=labels)"
      ],
      "metadata": {
        "id": "koR6DNJvjwyu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to distinguish the unique labels in our SER dataset\n",
        "if not labels_per_frame:\n",
        "  labels = [frame['labels'] for frame in train_dataset]\n",
        "else:\n",
        "  labels = [element for frame in train_dataset for element in frame['labels']]\n",
        "labels = np.array(labels)\n",
        "label_list = np.unique(labels)\n",
        "label_list.sort()  # Let's sort it for determinism\n",
        "num_labels = len(label_list)\n",
        "print(f\"A classification problem with {num_labels} classes: {label_list}\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# find the accuracy of just predicting the majority label\n",
        "prob_majority = float(counts.max())/len(labels)\n",
        "\n",
        "print(f'Calculating only the majority class {np.argmax(counts)} equals {prob_majority*100}%')\n",
        "print(unique, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdQukHDlj2YO",
        "outputId": "f307ab7c-d5a3-4b76-e2a0-35e65ecedccb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A classification problem with 8 classes: [0 1 2 3 4 5 6 7]\n",
            "Calculating only the majority class 0 equals 95.55900621118012%\n",
            "[0 1 2 3 4 5 6 7] [3077   21   32   23   27   11   10   19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to distinguish the unique labels in our SER dataset\n",
        "if not labels_per_frame:\n",
        "  labels = [frame['labels'] for frame in eval_dataset]\n",
        "else:\n",
        "  labels = [element for frame in eval_dataset for element in frame['labels']] #for element in song ]\n",
        "labels = np.array(labels)\n",
        "label_list = np.unique(labels)\n",
        "label_list.sort()  # Let's sort it for determinism\n",
        "num_labels = len(label_list)\n",
        "print(f\"A classification problem with {num_labels} classes: {label_list}\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# find the accuracy of just predicting the majority label\n",
        "prob_majority = float(counts.max())/len(labels)\n",
        "\n",
        "print(f'Calculating only the majority class {np.argmax(counts)} equals {prob_majority*100}%')\n",
        "print(unique, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWbcXsWtj2RO",
        "outputId": "4b1f4fa5-a977-4588-cc3c-dfdc7e74d054"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A classification problem with 8 classes: [0 1 2 3 4 5 6 7]\n",
            "Calculating only the majority class 0 equals 95.57971014492753%\n",
            "[0 1 2 3 4 5 6 7] [1319    9   13   10   12    4    5    8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "oS-llpO4kIIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the config from the base model (including all parameter)\n",
        "\n",
        "model_name_or_path = \"facebook/wav2vec2-base-960h\"\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,  # Use facebook/wav2vec2-base-960h as base config\n",
        "    num_labels=num_labels,  # the number of labels discovered above\n",
        "    problem_type='single_label_classification',\n",
        "    final_dropout=0.2\n",
        ")\n",
        "\n",
        "# set pooling to mean\n",
        "pooling_mode = \"mean\"\n",
        "setattr(config, 'pooling_mode', pooling_mode)"
      ],
      "metadata": {
        "id": "8rmH01T1kJPM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "89db950d8a02450bbde296420f0ea758",
            "e991af8d2360420d9348cb069fbf8935",
            "cb704324e48c475c89fb770ff68b2c76",
            "4f1f298290cf414f8648ae1e755443be",
            "d656d0a8462b4a27ba86677ba68e1939",
            "77b490f5669b48d4941d10420a501342",
            "83c8ceccb894496cb7e90af5975e50e1",
            "d1ba901ca8d548b4b512ff4decdacea3",
            "0d7b0416530e499daaf7463763c5e34a",
            "580e7f0bd98043af9fbd29e0021f3a6e",
            "d635eb2b8443433b9adf21720fe7a10f",
            "16533f9a0292401f827112f760bb7fa0",
            "1447e608cfe94b31b8a8618fe4412ef4",
            "bbde0ba842c54498ab6e4198259c8f81",
            "5fd109058bab4f84bc0f54eb72d34dc5",
            "863c89eef92a43a38837c5c4980d2e5e",
            "af0118557c8148f2af5f4bff0bd4d297",
            "691a60da77bb4b0f8186f98f361e9d71",
            "22ea46d2de964effb103b7748344c727",
            "50932d2cbd8c42508b505c3f8a624871",
            "54348acff36f4f298fc785ca35e2b1a0",
            "15ce44646b49405e967d333adc12e958",
            "9c81d123ed594f12a19456bfa6f253f5",
            "765c9584e0064446bf4807ab3731c70f",
            "9ba79ac5bc73426589242d2c87d6eaf9",
            "d78053385bc842d692c1a5f750f54568",
            "026b09b84fcb491b81154940aa057d58",
            "a5c9f5e4ceff475eba6c8ad6e471eb52",
            "002cb2e1e7fc4e5ba5200a6a4d28fa95",
            "9372650999c748ce915075f57a5553d4",
            "fbddd93b3806499d8566229f0bd70236",
            "67547ba68842451db20c3f4be07c6ade",
            "89dfa7280afd4dd2a35aac25179a89f3"
          ]
        },
        "id": "wuZv05o8kLfG",
        "outputId": "4e9a112e-b241-4228-cd0a-f38e7031f77e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89db950d8a02450bbde296420f0ea758",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16533f9a0292401f827112f760bb7fa0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c81d123ed594f12a19456bfa6f253f5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "cr0oaUD8l5A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "from transformers.file_utils import ModelOutput\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SpeechClassifierOutput(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n"
      ],
      "metadata": {
        "id": "M_oCbMyrl6Eq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Wav2Vec2ClassificationHead(nn.Module):\n",
        "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Dense Layers\n",
        "        # config.hidden_size is 768 by default\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dense1 = nn.Linear(config.hidden_size, 500)\n",
        "        self.dense2 = nn.Linear(76800, 500)\n",
        "        self.dense3 = nn.Linear(1000, 500)\n",
        "        self.dense4 = nn.Linear(500, 500)\n",
        "        self.dense5 = nn.Linear(500, 1024)\n",
        "        self.output = nn.Linear(500, config.num_labels)\n",
        "        \n",
        "\n",
        "        # BatchNormalization (1d and 2d)\n",
        "        self.batchnorm1d1 = nn.BatchNorm1d(config.hidden_size)\n",
        "        self.batchnorm1d2 = nn.BatchNorm1d(500)\n",
        "        self.batchnorm2d1 = nn.BatchNorm2d(20)\n",
        "        self.batchnorm2d2 = nn.BatchNorm2d(50)\n",
        "        self.norm = nn.LayerNorm(500)\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(config.final_dropout)\n",
        "\n",
        "        # Convolution\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=(5, 5), stride=1, padding=\"same\")\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5), stride=1, padding=\"same\")\n",
        "\n",
        "        # Pooling\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Activation\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        self.blstm_layer = nn.LSTM(input_size=256, hidden_size=config.hidden_size, batch_first=False, bidirectional=True)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, features, rgb_specs, **kwargs):\n",
        "\n",
        "        # features is the representation of the raw audio\n",
        "        x = features\n",
        "        \n",
        "        x = self.dense(x)\n",
        "        x = self.batchnorm1d1(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.dense1(x)\n",
        "        x = self.batchnorm1d2(x)\n",
        "        \n",
        "        # spectrogram\n",
        "        x1 = rgb_specs\n",
        "        x1 = x1.reshape(x1.shape[0], 1, x1.shape[1], x1.shape[2])\n",
        "        \n",
        "        x1 = self.conv1(x1)\n",
        "        x1 = self.pool(x1)\n",
        "        x1 = self.batchnorm2d1(x1)\n",
        "        \n",
        "        x1 = self.conv2(x1)\n",
        "        x1 = self.pool(x1)\n",
        "        x1 = self.batchnorm2d2(x1)\n",
        "\n",
        "        x1 = x1.reshape(x1.shape[0], x1.shape[1], x1.shape[2]*x1.shape[3])\n",
        "        x1, (_, _) = self.blstm_layer(x1)\n",
        "        \n",
        "        x1 = self.flatten(x1)\n",
        "        \n",
        "        x1 = self.dense2(x1)\n",
        "        x1 = self.dropout(x1)\n",
        "        x1 = self.relu(x1)\n",
        "        \n",
        "        out = torch.cat([x,x1], dim=1)\n",
        "        \n",
        "        out = self.dense3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = self.dense4(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = self.dense4(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = self.dense4(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        out = self.dense4(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.norm(out)\n",
        "        \n",
        "        if labels_per_frame:\n",
        "            out = self.dense5(out)\n",
        "            out = out.reshape(-1, 128, 8)\n",
        "        else:\n",
        "            out = self.output(out)\n",
        "            out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.pooling_mode = config.pooling_mode\n",
        "        self.config = config\n",
        "\n",
        "        self.wav2vec2 = Wav2Vec2Model(config)\n",
        "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
        "\n",
        "    def merged_strategy(\n",
        "            self,\n",
        "            hidden_states,\n",
        "            mode=\"mean\"\n",
        "    ):\n",
        "        if mode == \"mean\":\n",
        "            outputs = torch.mean(hidden_states, dim=1)\n",
        "        elif mode == \"sum\":\n",
        "            outputs = torch.sum(hidden_states, dim=1)\n",
        "        elif mode == \"max\":\n",
        "            outputs = torch.max(hidden_states, dim=1)[0]\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_values, rgb_specs,\n",
        "            attention_mask=None,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=None,\n",
        "            return_dict=None,\n",
        "            labels=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        outputs = self.wav2vec2(\n",
        "            input_values,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = outputs[0]\n",
        "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
        "        logits = self.classifier(hidden_states, rgb_specs)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            self.config.problem_type == \"single_label_classification\"\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            \n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SpeechClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "sagEoCrul8SG"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "The data is processed so that we are ready to start setting up the training pipeline. We will make use of 🤗's [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:\n",
        "\n",
        "- Define a data collator. In contrast to most NLP models, Wav2vec-VAD has a much larger input length than output length. *E.g.*, a normal window has 40960 samples but only 128 labels. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. \n",
        "\n",
        "- Evaluation metric. During training, the model should be evaluated on the accuracy per frame. A `compute_metrics` function is defined accordingly\n",
        "\n",
        "- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
        "\n",
        "- Define the training configuration.\n",
        "\n",
        "After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to detect speech."
      ],
      "metadata": {
        "id": "I8T2p7f9m0K5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set-up Trainer\n",
        "\n",
        "Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81).\n",
        "\n",
        "Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of XLSR-Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n",
        "Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss."
      ],
      "metadata": {
        "id": "RmwgwGZhm3d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Union\n",
        "import torch\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import transformers\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"], 'rgb_specs': feature['rgb_spectrogram']} for feature in features]\n",
        "        if labels_per_frame:\n",
        "            label_features = [frame for window in features for frame in window[\"labels\"]]\n",
        "            label_features = np.array(label_features).reshape(-1, 128)\n",
        "        else:\n",
        "            label_features = [window[\"labels\"] for window in features]\n",
        "            label_features = np.array(label_features)\n",
        "        d_type = torch.long\n",
        "        \n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
        "        return batch"
      ],
      "metadata": {
        "id": "0GxBVMx1m0ii"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "metadata": {
        "id": "CzN2NyH-m5so"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    print(p.predictions.shape)\n",
        "    \n",
        "    if labels_per_frame:\n",
        "        preds = p.predictions.reshape(p.predictions.shape[0]*p.predictions.shape[1], config.num_labels)\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        correct_labels = p.label_ids.reshape(p.label_ids.shape[0]*p.label_ids.shape[1])\n",
        "    else:\n",
        "        preds = p.predictions\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        correct_labels = p.label_ids\n",
        "    return {\"accuracy\": (preds == correct_labels).astype(np.float32).mean().item()}"
      ],
      "metadata": {
        "id": "v-Pxzr54m-BW"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjIIPlmtnEgE",
        "outputId": "fa25e335-b6fb-4dad-c1df-d3b9956b4dd7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.norm.weight', 'classifier.batchnorm1d2.running_mean', 'wav2vec2.masked_spec_embed', 'classifier.output.weight', 'classifier.norm.bias', 'classifier.blstm_layer.bias_ih_l0', 'classifier.blstm_layer.weight_ih_l0_reverse', 'classifier.dense2.weight', 'classifier.batchnorm2d1.running_mean', 'classifier.blstm_layer.bias_hh_l0', 'classifier.batchnorm2d1.weight', 'classifier.batchnorm2d1.bias', 'classifier.blstm_layer.weight_hh_l0', 'classifier.blstm_layer.weight_hh_l0_reverse', 'classifier.conv2.bias', 'classifier.dense1.weight', 'classifier.batchnorm2d2.running_mean', 'classifier.dense.weight', 'classifier.blstm_layer.weight_ih_l0', 'classifier.dense3.weight', 'classifier.dense5.weight', 'classifier.batchnorm2d1.running_var', 'classifier.batchnorm1d2.num_batches_tracked', 'classifier.batchnorm2d2.num_batches_tracked', 'classifier.batchnorm1d1.running_var', 'classifier.dense3.bias', 'classifier.batchnorm1d2.bias', 'classifier.blstm_layer.bias_hh_l0_reverse', 'classifier.batchnorm2d2.bias', 'classifier.conv1.bias', 'classifier.batchnorm1d2.running_var', 'classifier.dense4.weight', 'classifier.batchnorm2d2.running_var', 'classifier.conv1.weight', 'classifier.dense4.bias', 'classifier.batchnorm1d1.num_batches_tracked', 'classifier.dense1.bias', 'classifier.batchnorm1d2.weight', 'classifier.batchnorm1d1.weight', 'classifier.batchnorm2d2.weight', 'classifier.conv2.weight', 'classifier.dense2.bias', 'classifier.dense5.bias', 'classifier.batchnorm1d1.bias', 'classifier.output.bias', 'classifier.dense.bias', 'classifier.blstm_layer.bias_ih_l0_reverse', 'classifier.batchnorm1d1.running_mean', 'classifier.batchnorm2d1.num_batches_tracked']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first component of Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretraining and does not need to be fine-tuned anymore. \n",
        "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part."
      ],
      "metadata": {
        "id": "2np1v8-snGye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.freeze_feature_extractor()"
      ],
      "metadata": {
        "id": "ntelRYrdnIHW"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a final step, we define all parameters related to training. \n",
        "\n",
        "For more explanations on the possible parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
        "\n",
        "**Note**: If one wants to save the trained models in his/her google drive the commented-out `output_dir` can be used instead."
      ],
      "metadata": {
        "id": "mq4o1zd3nJ-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time = datetime.now()\n",
        "model_path = f\"{path_files}Models/wav2vec2_emotion_unet_labels_{snr.value}/{time}\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    per_device_train_batch_size=batch_size,  #256\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=20,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=save_best,\n",
        "    num_train_epochs=train_epochs,\n",
        "    fp16=True,\n",
        "    save_steps=1000,\n",
        "    eval_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=1e-4,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "sCoGez_3nEwI"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if is_apex_available():\n",
        "    from apex import amp\n",
        "\n",
        "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
        "    _is_native_amp_available = True\n",
        "    from torch.cuda.amp import autocast\n",
        "\n",
        "\n",
        "class CTCTrainer(Trainer):\n",
        "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform a training step on a batch of inputs.\n",
        "\n",
        "        Subclass and override to inject custom behavior.\n",
        "\n",
        "        Args:\n",
        "            model (:obj:`nn.Module`):\n",
        "                The model to train.\n",
        "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
        "                The inputs and targets of the model.\n",
        "\n",
        "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
        "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
        "\n",
        "        Return:\n",
        "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
        "        \"\"\"\n",
        "\n",
        "        model.train()\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        if self.use_amp:\n",
        "            with autocast():\n",
        "                loss = self.compute_loss(model, inputs)\n",
        "        else:\n",
        "            loss = self.compute_loss(model, inputs)\n",
        "\n",
        "        if self.args.gradient_accumulation_steps > 1:\n",
        "            loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "        if self.use_amp:\n",
        "            self.scaler.scale(loss).backward()\n",
        "        elif self.use_apex:\n",
        "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        elif self.deepspeed:\n",
        "            self.deepspeed.backward(loss)\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        return loss.detach()\n"
      ],
      "metadata": {
        "id": "aqpYHj0incy6"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CTCTrainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SScPnRmngk6",
        "outputId": "52f2c9b9-2d22-48cf-d428-b36ecf271a81"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Fup9l-bgnlAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training will take between several hours depending on the GPU allocated to this notebook. \n",
        "\n",
        "In case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (right mouse click -> inspect -> Console tab and insert code)."
      ],
      "metadata": {
        "id": "YJI6cp3fnmSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ],
      "metadata": {
        "id": "11PunSP2nsZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.end_run()"
      ],
      "metadata": {
        "id": "hVckP1pHnilv"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "X9g47nFpnuoP",
        "outputId": "ac836ffc-0427-4a72-bdcb-a901ce2ab906"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3220\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 640\n",
            "  Gradient Accumulation steps = 20\n",
            "  Total optimization steps = 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/50 02:43 < 07:21, 0.08 it/s, Epoch 2.79/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.326924</td>\n",
              "      <td>0.955797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.523800</td>\n",
              "      <td>1.319037</td>\n",
              "      <td>0.955797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1380\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1380, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to drive/MyDrive/Master Thesis/Models/wav2vec2_emotion_unet_labels_n-05/2022-02-23 12:52:21.058222/checkpoint-5\n",
            "Configuration saved in drive/MyDrive/Master Thesis/Models/wav2vec2_emotion_unet_labels_n-05/2022-02-23 12:52:21.058222/checkpoint-5/config.json\n",
            "Model weights saved in drive/MyDrive/Master Thesis/Models/wav2vec2_emotion_unet_labels_n-05/2022-02-23 12:52:21.058222/checkpoint-5/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1380\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1380, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to drive/MyDrive/Master Thesis/Models/wav2vec2_emotion_unet_labels_n-05/2022-02-23 12:52:21.058222/checkpoint-10\n",
            "Configuration saved in drive/MyDrive/Master Thesis/Models/wav2vec2_emotion_unet_labels_n-05/2022-02-23 12:52:21.058222/checkpoint-10/config.json\n",
            "Model weights saved in drive/MyDrive/Master Thesis/Models/wav2vec2_emotion_unet_labels_n-05/2022-02-23 12:52:21.058222/checkpoint-10/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                 if (\n",
            "\u001b[0;32m<ipython-input-87-19cfebbea6da>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "_IMRKurCnyQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAD_predictions = 'Wav2vec'"
      ],
      "metadata": {
        "id": "h1RrOuMtp7xL"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vad_files_test = []\n",
        "if VAD_predictions == 'Wav2vec':\n",
        "    with open(f'{path_files}wav_files_test_wav2vec.txt') as file_wav:\n",
        "        for line in file_wav:\n",
        "            vad_files_test.append(line.strip())\n",
        "    with open(f'{path_files}predictions_wav2vec.p', 'rb') as file_wav:\n",
        "      predictions_wav2vec = pickle.load(file_wav)\n",
        "if VAD_predictions == 'Unet':\n",
        "    with open(f'{path_files}wav_files_test_unet.txt') as file_wav:\n",
        "        for line in file_wav:\n",
        "            vad_files_test.append(line.strip())\n",
        "    with open(f'{path_files}predictions_unet.p', 'rb') as file_wav:\n",
        "      predictions_wav2vec = pickle.load(file_wav)"
      ],
      "metadata": {
        "id": "ydkYECennyEm"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_test = 100 #nr_test_files #len(wav_files_test)\n",
        "features_test, y_test, frames_pos_test, frames_test = get_feature_and_labels(vad_files_test[0:end_test], parameter, emotion=True)\n",
        "plot_file_distribution(wav_files_test[0:end_test], y_test)"
      ],
      "metadata": {
        "id": "WzqatKZ7n3jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = get_processed_frames(frames_test, features_test, y_test, emotion=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8Ow_CzooJGR",
        "outputId": "37d30bbb-93d1-4bb1-cf90-1732e87181c2"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading feature extractor configuration file https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/07e398f6c4f4eb4f676c75befc5ace223491c79cea1109fb4029751892d380a1.bc3155ca0bae3a39fc37fc6d64829c6a765f46480894658bb21c08db6155358d\n",
            "Feature extractor Wav2Vec2FeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"return_attention_mask\": false,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The target sampling rate: 16000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4600/4600 [00:00<00:00, 66735.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to distinguish the unique labels in our SER dataset\n",
        "if not labels_per_frame:\n",
        "  labels = [frame['labels'] for frame in test_dataset]\n",
        "else:\n",
        "  labels = [element for frame in test_dataset for element in frame['labels']]\n",
        "labels = np.array(labels)\n",
        "label_list = np.unique(labels)\n",
        "label_list.sort()  # Let's sort it for determinism\n",
        "num_labels = len(label_list)\n",
        "print(f\"A classification problem with {num_labels} classes: {label_list}\")\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "prob_majority = float(counts.max())/len(labels)\n",
        "# if prob_majority < 0.5:\n",
        "    # prob_majority = 1-prob_majority\n",
        "print(f'Calculating only the majority class {np.argmax(counts)} equals {prob_majority*100}%')\n",
        "print(unique, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLJZzEjCoNUT",
        "outputId": "850e5634-4a09-4808-cf42-fa47b71e9f36"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A classification problem with 8 classes: [0 1 2 3 4 5 6 7]\n",
            "Calculating only the majority class 0 equals 95.95652173913042%\n",
            "[0 1 2 3 4 5 6 7] [4414   27   27   36   36   21   15   24]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_wav2vec = predictions_wav2vec.mean(axis=1)"
      ],
      "metadata": {
        "id": "-vp5u57AqwI4"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_wav2vec = copy.deepcopy(test_dataset)\n",
        "subset_wav2vec = [element for index, element in enumerate(test_dataset_wav2vec) if predictions_wav2vec[index] > 0.5]\n",
        "if not labels_per_frame:\n",
        "    for index, element in enumerate(subset_wav2vec):\n",
        "        # as no 0 is in the remaining data, all values need to be reduced by one to start at 0 for the labels\n",
        "        if element['labels'] == 0:\n",
        "            # print(index)\n",
        "            subset_wav2vec[index]['labels'] = 1\n",
        "            subset_wav2vec[index]['majority_label'] = 1\n",
        "        subset_wav2vec[index]['majority_label'] = element['majority_label']\n",
        "\n",
        "labels_wav2vec = [s['labels'] for s in subset_wav2vec]"
      ],
      "metadata": {
        "id": "UiR7BdAWojOJ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(subset_wav2vec)\n",
        "labels_test = labels_wav2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "_io6U7e0pXe0",
        "outputId": "925c075b-4c57-407e-cd9f-5bb4d18dd8f4"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 2119\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/50 02:43 < 07:21, 0.08 it/s, Epoch 2.79/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.326924</td>\n",
              "      <td>0.955797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.523800</td>\n",
              "      <td>1.319037</td>\n",
              "      <td>0.955797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='71' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 14:36]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2119, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C1YmYR8Kq3bf"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if labels_per_frame:\n",
        "    predictions = np.argmax(predictions.predictions, axis=2)\n",
        "    predictions = predictions.reshape(-1)\n",
        "else:\n",
        "    predictions = np.argmax(predictions.predictions, axis=1)"
      ],
      "metadata": {
        "id": "WrUYy0fCpbze"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = np.array(labels_test).reshape(-1)"
      ],
      "metadata": {
        "id": "sYm7xDvGpe1B"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(labels_test, predictions, target_names=['calm/neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']))"
      ],
      "metadata": {
        "id": "zi_3jWTxpgow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cf_matrix = confusion_matrix(labels_test, predictions)\n",
        "print(cf_matrix/cf_matrix.sum())\n",
        "plt.figure(figsize= (16,9))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(cf_matrix/cf_matrix.sum(), annot=True, \n",
        "            xticklabels=[ 'no speech', 'calm/neutral', 'happy', 'sad', 'angry', \n",
        "                         'fearful', 'disgust', 'surprised'], \n",
        "            yticklabels=['no speech', 'calm/neutral', 'happy', 'sad', 'angry', \n",
        "                         'fearful', 'disgust', 'surprised'], fmt='.2%')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.savefig(model_path + '/confusion_matrix_no_speech_added.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jg00-G8hoj2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8VdZn56Xry5G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}